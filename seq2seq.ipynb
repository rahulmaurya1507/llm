{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw65YInX_wPD",
    "outputId": "4bf69790-6d98-4273-ff11-5a1a617a8f61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi8IDbUH_wPE"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZHzWjo8_wPF"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [src_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [src_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs: [src_len, batch_size, hidden_dim * n_directions]\n",
    "        # hidden, cell: [n_layers, batch_size, hidden_dim]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70nfIIrV_wPF"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [1, batch_size] (we're processing one time step at a time)\n",
    "        input = input.unsqueeze(0)\n",
    "        # embedded: [1, batch_size, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output: [1, batch_size, hidden_dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction: [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRI7Tllt_wPG"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src_len, batch_size]\n",
    "        # trg: [trg_len, batch_size]\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvomFpZ__wPG"
   },
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXnwNqNI_wPG"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2q1ru-i__wPH",
    "outputId": "68f6c98a-e00b-451d-80b1-23daed9d8b40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'In reply, Pakistan got off to a solid start.'\n",
    "tokens = [tok.text for tok in spacy_en.tokenizer(txt)]\n",
    "src_voc = build_vocab_from_iterator(\n",
    "            [tokens],\n",
    "            specials=['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        )\n",
    "src_voc.set_default_index(src_voc['<unk>'])\n",
    "src_voc['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_n01doOx_wPH"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.src_vocab = build_vocab_from_iterator(\n",
    "            (self.tokenizer(src) for src, trg in data),\n",
    "            specials=['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        )\n",
    "\n",
    "        self.trg_vocab = build_vocab_from_iterator(\n",
    "            (self.tokenizer(trg) for src, trg in data),\n",
    "            specials=['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        )\n",
    "        self.src_vocab.set_default_index(self.src_vocab['<unk>'])\n",
    "        self.trg_vocab.set_default_index(self.trg_vocab['<unk>'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src, trg = self.data[index]\n",
    "        src_tokens = [self.src_vocab['<sos>']] + [self.src_vocab[token] for token in self.tokenizer(src)] + [self.src_vocab['<eos>']]\n",
    "        trg_tokens = [self.trg_vocab['<sos>']] + [self.trg_vocab[token] for token in self.tokenizer(trg)] + [self.trg_vocab['<eos>']]\n",
    "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYu3pW60_wPH"
   },
   "outputs": [],
   "source": [
    "def _load(file):\n",
    "    with open(file, 'r') as handle:\n",
    "        return [line.strip() for line in handle.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LFLUHWE_wPH"
   },
   "outputs": [],
   "source": [
    "src_path = 'en-hi/train.en'\n",
    "tar_path = 'en-hi/train.hi'\n",
    "\n",
    "english = _load(src_path)[:10000]\n",
    "hindi = _load(tar_path)[:10000]\n",
    "\n",
    "data = list(zip(english, hindi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTd7c4HV_wPI"
   },
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(data, tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=dataset.src_vocab['<pad>'])\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=dataset.trg_vocab['<pad>'])\n",
    "    return src_batch, trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTDzVzol_wPI"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP9iMFtc_wPI"
   },
   "outputs": [],
   "source": [
    "input_dim = len(dataset.src_vocab)\n",
    "output_dim = len(dataset.trg_vocab)\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "n_epochs = 100\n",
    "clip = 1\n",
    "batch_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim, n_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "A7pjpU8HD6-Q",
    "outputId": "abb86c0e-59fc-4b7c-d7a0-471ef6937dba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO4TRA4U_wPI"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.trg_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZSUkykSa2As"
   },
   "outputs": [],
   "source": [
    "# Define the proportion for training and validation splits\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size   # 20% for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for both sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc2PI5bYc1Kr"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg.transpose(0, 1).contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, trg)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions and mask padding tokens\n",
    "            pred_tokens = output.argmax(dim=1)\n",
    "            non_pad_mask = trg != dataset.trg_vocab['<pad>']\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_predictions += (pred_tokens[non_pad_mask] == trg[non_pad_mask]).sum().item()\n",
    "            total_predictions += non_pad_mask.sum().item()\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    avg_accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9jcx_OGo_wPI",
    "outputId": "1e0eb577-f3c9-4230-f3eb-457c05d14087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [300], Loss: 7.4271, Accuracy: 4.31%\n",
      "Epoch [1/100], Step [600], Loss: 7.3259, Accuracy: 4.35%\n",
      "Epoch [1/100], Step [900], Loss: 7.2869, Accuracy: 4.39%\n",
      "Epoch [1/100] completed, Average Loss: 7.2758, Average Accuracy: 4.41%\n",
      "Validation Loss: 7.1800, Validation Accuracy: 4.53%\n",
      "Validation Loss: 7.1800, Validation Accuracy: 4.53%\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for idx, (src, trg) in enumerate(train_dataloader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, trg)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg = trg.transpose(0, 1).contiguous().view(-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        pred = output.argmax(dim=1)\n",
    "        non_pad_mask = trg != dataset.trg_vocab['<pad>']\n",
    "        correct_predictions += (pred[non_pad_mask] == trg[non_pad_mask]).sum().item()\n",
    "        total_predictions += non_pad_mask.sum().item()\n",
    "\n",
    "        if idx % 300 == 0 and idx > 0:\n",
    "            avg_loss = epoch_loss / (idx + 1)\n",
    "            avg_accuracy = correct_predictions / total_predictions * 100\n",
    "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{idx}], Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    # End of epoch training stats\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}] completed, Average Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.2f}%')\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy = evaluate_model(model, val_dataloader, criterion, device)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctfChsyXbTqP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
