{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEdWpIKI_Srj"
      },
      "source": [
        "# Understanding the Transformer Architecture\n",
        "In this tutorial, we'll delve into the Transformer architecture, a groundbreaking model in the NLP domain. We'll use PyTorch to implement the model and understand its components.\n",
        "\n",
        "##Prerequisites:\n",
        "Basic understanding of PyTorch.\n",
        "Familiarity with deep learning concepts.\n",
        "##Setting up the Environment\n",
        "First, let's set up our Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMfr2f0Q_ZcT",
        "outputId": "689a9af4-7c39-43ba-fc26-c80f729bb3cf"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQXVwNw0_gmv"
      },
      "source": [
        "## 1. Self-Attention Mechanism\n",
        "The self-attention mechanism allows the model to weigh the importance of different words in a sequence relative to a particular word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XPJVm8nl_na7"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        # Store attention weights\n",
        "        self.attention_weights = attention\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "        out = out.reshape(N, query_len, self.heads * self.head_dim)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4z0w8dH_pAO"
      },
      "source": [
        "## 2. Transformer Block\n",
        "The Transformer block consists of the self-attention mechanism and a feed-forward neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "htCgui4D_slX"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add & norm\n",
        "        x = self.norm1(attention + query)\n",
        "        x = self.dropout(x)\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(forward + x)\n",
        "        out = self.dropout(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZyxjpoQ_wDU"
      },
      "source": [
        "## 3. Encoder\n",
        "The encoder consists of multiple Transformer blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BhV-a_26_w3t"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7irf4YgP_ydL"
      },
      "source": [
        "## 4. Decoder\n",
        "The decoder also consists of multiple Transformer blocks but also has an additional feed-forward neural network at the end to produce predicted tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5skIwph4_0Wz"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR30il27__4z"
      },
      "source": [
        "## 5. Transformer\n",
        "Finally, the Transformer model combines the encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0_hXZMmn_5T-"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"cuda\",\n",
        "        max_length=100,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVCdnMFjAnQ4"
      },
      "source": [
        "Let's put together a simple demonstration using the Transformer architecture for a sequence-to-sequence task. For simplicity, we'll use a toy task: reversing a sequence. This will allow us to focus on the architecture and its visualization rather than the complexities of a real-world dataset.\n",
        "\n",
        "This code will train the Transformer model to reverse sequences and then visualize the attention weights for the first sequence in the dataset. The heatmaps will show how each head in the multi-head attention mechanism focuses on different parts of the source sequence to produce the target sequence.\n",
        "\n",
        "**Note: **This is a toy example, and the Transformer architecture is designed for more complex tasks. However, this demonstration should give you a basic understanding of how to train the model and visualize its inner workings.\n",
        "\n",
        "## Step-by-step Implementation:\n",
        "### Data Preparation:\n",
        "We'll generate sequences of integers and their reversed counterparts as source and target sequences.\n",
        "\n",
        "### Model Training:\n",
        "We'll train the Transformer model to learn this sequence reversal task.\n",
        "\n",
        "### Visualization:\n",
        "We'll visualize the attention weights to understand how the model is making its predictions.\n",
        "\n",
        "The visualization displays the attention weights of each head in the multi-head attention mechanism. Each heatmap corresponds to one head.\n",
        "\n",
        "**X-axis:** Represents the positions in the source sequence (e.g., each word or token in the input sentence).\n",
        "\n",
        "**Y-axis:** Represents the positions in the target sequence (e.g., each word or token in the output sentence).\n",
        "\n",
        "**Color:** The color in each cell of the heatmap indicates the attention weight. Darker colors (e.g., closer to purple in the \"viridis\" colormap) mean higher attention weights, indicating that when producing the word at the corresponding Y position, the model pays more attention to the word at the X position.\n",
        "\n",
        "#### Interpretation:\n",
        "By examining the heatmap:\n",
        "\n",
        "You can see which parts of the input sequence (source) the model focuses on when producing each word/token of the output sequence (target).\n",
        "\n",
        "If the model is working correctly, you might observe patterns. For instance, in a translation task, you might see attention aligning to similar-meaning words or phrases between source and target.\n",
        "\n",
        "In our toy example of reversing a sequence, an ideal attention pattern would probably show a diagonal pattern from the top-right to the bottom-left, indicating that each output token is primarily attending to its opposite counterpart in the input.\n",
        "\n",
        "#### Why is this Useful?\n",
        "Visualizing attention weights can provide insights into how the model is making its decisions. It can help in:\n",
        "\n",
        "**Debugging:** If the model isn't performing well, the attention patterns might show where it's going wrong.\n",
        "\n",
        "**Interpretability:** Even if the model performs well, it's beneficial to understand how it's making decisions, especially in critical applications.\n",
        "\n",
        "**Model Improvement:** By understanding attention patterns, one might get ideas on how to improve the model, data preprocessing, or post-processing.\n",
        "\n",
        "In summary, the attention visualization provides a window into the inner workings of the Transformer model, helping us understand its decision-making process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preparation\n",
        "def generate_data(num_samples=10):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        seq = torch.randint(0, 10, (10,))\n",
        "        rev_seq = torch.flip(seq, [0])\n",
        "        data.append((seq, rev_seq))\n",
        "    return data\n",
        "\n",
        "generate_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "wBvHczEEArTM",
        "outputId": "51d2b3a6-3f2c-46d8-c377-4dd477f3e47d"
      },
      "outputs": [],
      "source": [
        "# Assuming all the previous classes (SelfAttention, TransformerBlock, Encoder, Decoder, Transformer) are already defined\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "SRC_VOCAB_SIZE = 11  # 0-9 digits and padding\n",
        "TRG_VOCAB_SIZE = 11  # 0-9 digits and padding\n",
        "EMBED_SIZE = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "DROPOUT = 0.10\n",
        "MAX_LENGTH = 5\n",
        "FORWARD_EXPANSION = 4\n",
        "SRC_PAD_IDX = 10\n",
        "TRG_PAD_IDX = 10\n",
        "\n",
        "\n",
        "data = generate_data()\n",
        "\n",
        "# Model, Optimizer, and Loss\n",
        "model = Transformer(\n",
        "    SRC_VOCAB_SIZE,\n",
        "    TRG_VOCAB_SIZE,\n",
        "    SRC_PAD_IDX,\n",
        "    TRG_PAD_IDX,\n",
        "    EMBED_SIZE,\n",
        "    NUM_ENCODER_LAYERS,\n",
        "    FORWARD_EXPANSION,\n",
        "    NUM_HEADS,\n",
        "    DROPOUT,\n",
        "    device,\n",
        "    MAX_LENGTH,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "\n",
        "# Training Loop with tqdm progress bar\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(data, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=False)\n",
        "    for src, trg in progress_bar:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src.unsqueeze(0), trg.unsqueeze(0))\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': total_loss / (len(data) / (BATCH_SIZE if BATCH_SIZE < len(data) else len(data)))})\n",
        "\n",
        "# Visualization\n",
        "def visualize_attention(src, trg, model):\n",
        "    src, trg = src.to(device), trg.to(device)\n",
        "    output = model(src.unsqueeze(0), trg.unsqueeze(0))\n",
        "\n",
        "    # Access the attention weights from the first decoder layer\n",
        "    attention = model.decoder.layers[0].attention.attention_weights.squeeze(0)\n",
        "\n",
        "    fig, axs = plt.subplots(1, NUM_HEADS, figsize=(15, 10))\n",
        "    for i, ax in enumerate(axs):\n",
        "        sns.heatmap(attention[i].cpu().detach().numpy(), ax=ax, cmap=\"viridis\")\n",
        "        ax.set_title(f\"Head {i + 1}\")\n",
        "        ax.set_xlabel(\"Source Sequence\")\n",
        "        ax.set_ylabel(\"Target Sequence\")\n",
        "    plt.show()\n",
        "\n",
        "# Test and visualize\n",
        "src, trg = data[0]\n",
        "print(\"Source Sequence:\", src)\n",
        "print(\"Target Sequence:\", trg)\n",
        "visualize_attention(src, trg, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SquVgFzL_5lb"
      },
      "source": [
        "## Conclusion\n",
        "This tutorial provided a detailed look into the Transformer architecture using PyTorch. You can now extend this base model to create state-of-the-art models like BERT and GPT!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
